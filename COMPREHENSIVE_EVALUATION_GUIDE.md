# DEEMERGE Comprehensive Evaluation Guide

## ğŸ¯ **Project Overview**

DEEMERGE is a **comprehensive LLM evaluation engine** that benchmarks and compares the performance of **ALL major language models** for Slack message topic detection and analysis.

## ğŸ¢ **Supported Models**

### **Cloud APIs (Require API Keys):**
- **OpenAI**: GPT-4, GPT-3.5-turbo, GPT-4o
- **Google**: Gemini Pro, Gemini 1.5 Pro, Gemini 1.5 Flash
- **Groq**: Llama3-8B, Llama3-70B, Mixtral-8x7B
- **Anthropic**: Claude-3 Opus, Claude-3 Sonnet, Claude-3 Haiku

### **Local Models (No API Keys Required):**
- **Google Gemma**: 2B, 7B, 2B-IT, 7B-IT (local inference)

## ğŸš€ **Quick Start**

### **Option 1: Demo Mode (No API Keys Required)**
```bash
# Test the framework with simulated models
python test_evaluation_framework.py
```

### **Option 2: Full Evaluation (Requires API Keys)**

#### **Step 1: Setup Environment**
```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

#### **Step 2: Get API Keys**
- **OpenAI**: https://platform.openai.com/api-keys
- **Google**: https://makersuite.google.com/app/apikey
- **Groq**: https://console.groq.com/keys
- **Anthropic**: https://console.anthropic.com/

#### **Step 3: Set Environment Variables**
```bash
export OPENAI_API_KEY="your-openai-key"
export GOOGLE_API_KEY="your-google-key"
export GROQ_API_KEY="your-groq-key"
export ANTHROPIC_API_KEY="your-anthropic-key"
```

#### **Step 4: Run Evaluation**

**All Models (Recommended):**
```bash
python run_all_models_evaluation.py
```

**Individual Providers:**
```bash
# All cloud APIs
python model_evaluation.py

# Google Gemini only
python google_model_eval.py

# Local Gemma models
python gemma_eval.py
```

## ğŸ“Š **Evaluation Metrics**

Each model is evaluated on:
- **Topic Detection Accuracy**: Jaccard similarity for topic overlap
- **Action Item Extraction**: Precision, recall, F1 scores
- **JSON Response Validity**: Success rate of structured outputs
- **Performance**: Response time and token usage
- **Cost Analysis**: API costs per request

## ğŸ“ **Output Files**

### **Demo Mode:**
- `demo_evaluation_results.json` - Simulated model results
- `demo_evaluation_report.json` - Demo performance summary

### **Full Evaluation:**
- `comprehensive_model_evaluation.json` - All model results
- `model_evaluation_report.json` - Performance summary
- `final_evaluation_summary.json` - Master summary
- Individual model outputs in `output_*` files

## ğŸ”§ **Troubleshooting**

### **Missing Dependencies**
```bash
# Install missing packages
pip install torch transformers accelerate
pip install google-generativeai groq anthropic
```

### **API Key Issues**
```bash
# Check if keys are set
echo $OPENAI_API_KEY
echo $GOOGLE_API_KEY
echo $GROQ_API_KEY
echo $ANTHROPIC_API_KEY
```

### **Memory Issues (Local Models)**
```bash
# For Gemma models, ensure sufficient RAM
# 2B models: ~4GB RAM
# 7B models: ~16GB RAM
```

### **File Path Issues**
```bash
# Ensure you're in the correct directory
cd deemerge_test
pwd  # Should show /path/to/deemerge/deemerge_test
```

## ğŸ“ˆ **Expected Results**

### **Performance Rankings (Typical):**
1. **Claude-3 Opus** - Highest accuracy, highest cost
2. **GPT-4** - High accuracy, high cost
3. **Gemini 1.5 Pro** - Good accuracy, moderate cost
4. **Claude-3 Sonnet** - Good accuracy, moderate cost
5. **GPT-3.5-turbo** - Moderate accuracy, low cost
6. **Local Gemma** - Variable accuracy, no cost

### **Cost Comparison (per 1K tokens):**
- **OpenAI GPT-4**: ~$0.03-0.06
- **Claude-3 Opus**: ~$0.015-0.075
- **Gemini Pro**: ~$0.0005-0.0015
- **Groq**: ~$0.0001-0.0008
- **Local Gemma**: $0.00

## ğŸ¯ **Use Cases**

### **Research & Development:**
- Model selection for specific tasks
- Performance benchmarking
- Cost optimization analysis

### **Production Deployment:**
- Choose best model for your use case
- Balance accuracy vs. cost
- Consider privacy requirements

### **Academic Research:**
- Reproducible evaluation framework
- Standardized metrics across models
- Comparative analysis

## ğŸ” **Advanced Usage**

### **Custom Model Testing**
```python
# Add custom models to model_evaluation.py
CUSTOM_MODELS = {
    "your-provider": {
        "your-model": {"provider": "your-provider", "model": "your-model"}
    }
}
```

### **Batch Evaluation**
```bash
# Run evaluation with specific models only
python model_evaluation.py --models openai,google
```

### **Custom Prompts**
```bash
# Modify prompts in prompts/ directory
# Then run evaluation with custom prompts
```

## ğŸ“‹ **File Structure**

```
deemerge_test/
â”œâ”€â”€ ğŸ§ª EVALUATION SCRIPTS
â”‚   â”œâ”€â”€ test_evaluation_framework.py    # Demo mode (no API keys)
â”‚   â”œâ”€â”€ model_evaluation.py             # All cloud APIs
â”‚   â”œâ”€â”€ google_model_eval.py            # Google Gemini
â”‚   â”œâ”€â”€ gemma_eval.py                  # Local Gemma
â”‚   â””â”€â”€ run_all_models_evaluation.py    # Master script
â”œâ”€â”€ ğŸ”§ CORE ENGINE
â”‚   â”œâ”€â”€ src/run_approach1.py           # Single-step detection
â”‚   â”œâ”€â”€ src/run_approach2.py           # Multi-step pipeline
â”‚   â””â”€â”€ src/eval/                      # Evaluation tools
â”œâ”€â”€ ğŸ“ PROMPTS
â”‚   â”œâ”€â”€ approach1_single_prompt.txt
â”‚   â””â”€â”€ approach2_*.txt
â”œâ”€â”€ ğŸ“Š DATA
â”‚   â”œâ”€â”€ Synthetic_Slack_Messages.csv
â”‚   â””â”€â”€ benchmark_topics_corrected_fixed.json
â””â”€â”€ ğŸ“‹ CONFIG
    â”œâ”€â”€ config.py                      # Settings
    â””â”€â”€ requirements.txt               # Dependencies
```

## ğŸ‰ **Success Indicators**

âœ… **Framework working correctly when you see:**
- Demo runs without errors
- JSON files generated in output directory
- Performance metrics calculated
- Cost analysis completed

âœ… **Ready for production when:**
- All API keys configured
- Models tested and ranked
- Best model selected for your use case
- Cost-benefit analysis complete

## ğŸ†˜ **Getting Help**

1. **Run demo first**: `python test_evaluation_framework.py`
2. **Check API keys**: Ensure all required keys are set
3. **Verify dependencies**: `pip list | grep -E "(torch|transformers|openai|google)"`
4. **Check file paths**: Ensure you're in the `deemerge_test` directory

---

**ğŸ¯ Goal**: Find the best performing LLM for your Slack topic detection needs!
