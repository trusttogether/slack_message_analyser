{
  "provider": "openai",
  "model": "gpt-3.5-turbo",
  "response": "Error: Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 23975 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}",
  "parsed_response": "Error: Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 23975 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}",
  "json_valid": false,
  "duration": 0.20656085014343262,
  "timestamp": 1755659206.5964513
}